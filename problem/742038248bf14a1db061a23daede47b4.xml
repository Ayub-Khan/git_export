<problem display_name="Exercise 1" markdown="null">
  <text>
    <p>Imagine we implement a dictionary, all of whose keys are U.S. social security numbers (9 digit numbers). If we used a list with [mathjaxinline]10^9[/mathjaxinline] elements to represent the dictionary, we could do lookups in constant time.  Of course, if the dictionary contained entries for only 20 people (or, for that matter, only [mathjaxinline]3 * 10^8[/mathjaxinline] people), this would waste quite a lot of space.</p>
    <p>Which gets us to the subject of <b>hash functions</b>. A hash function maps a large space of inputs (e.g., all natural numbers) to a <i>smaller</i> space of outputs (e.g., the natural numbers between 0 and 5000). The space of possible outputs should be much smaller than the space of possible inputs!</p>
    <p>A hash function is "many-to-one", that is, multiple different inputs are mapped to the same output.  When two inputs are mapped to the same output, it is called a <b>collision</b>.  A good hash function  produces a uniform distribution, i.e., every output in the range is equally probable, which minimizes the probability of collisions.</p>
    <p>Remember the <a href="/static/lectureCode/intDict.py" target="_blank">intDict</a> code from the previous video? <code>intDict</code>
      uses a simple hash function (modulus) to implement a dictionary with integers as keys.  The basic idea is to represent instances of class <code>intDict</code> by a list of buckets, where each bucket is a list of (key, value) tuples. By making each bucket a list, we handle collisions by storing all of the values that hash to that bucket.</p>
    <p>Collisions are inevitable when implementing hash tables, because generally we are mapping a really big set of inputs to a much smaller set of buckets. Thinking about the way that <code>intDict</code> implements hashing - making each bucket a list, and assigning each element to one bucket's list - what is the expected average length of the list in each bucket of the hash table when the hash table is 10 buckets big for the following number of unique insertions (that is, no two elements inserted are equal)?</p>
    <ol>
      <li>
        <p>Hash table size = 10 buckets; number of unique insertions = 10</p>
        <p>Expected <i>average</i> length of the list for each bucket =  <stringresponse answer="1"><textline size="20"/></stringresponse> </p>
      </li>
      <li>
        <p>Hash table size = 10 buckets; number of unique insertions = 20</p>
        <p>Expected <i>average</i> length of the list for each bucket =  <stringresponse answer="2"><textline size="20"/></stringresponse> </p>
      </li>
      <li>
        <p>Hash table size = 10 buckets; number of unique insertions = 100</p>
        <p>Expected <i>average</i> length of the list for each bucket =  <stringresponse answer="10"><textline size="20"/></stringresponse> </p>
      </li>
      <p>
        <solution>
          <div class="detailed-solution">
            <p>
              <b>Explanation:</b>
            </p>
            <p>The average length of the list for each bucket will be the number of elements inserted divided by the number of buckets available (this is assuming tha we have a good hash function)</p>
          </div>
        </solution>
      </p>
      <p>There are many other ways to handle collisions, some considerably more efficient than using lists. However, this is probably the simplest mechanism, and it works fine if the hash table is big enough and the hash function provides a good enough approximation to a uniform distribution.</p>
      <li>
        <p>Consider the class <code>intDict</code>, particulary the method <code>getValue</code>. If there are no collisions, what would the complexity of <code>getValue</code> be?</p>
        <p>
          <optionresponse>
            <optioninput options="('O(1)','O(log(n))','O(n)','O(n log(n))','O(n^2)')" correct="O(1)"/>
          </optionresponse>
        </p>
      </li>
      <solution>
        <div class="detailed-solution">
          <p>
            <b>Explanation:</b>
          </p>
          <p>Without collisions, each key has only one value associated with it.</p>
        </div>
      </solution>
      <p>Obviously, the drawback of making a hash table so huge there's no collisions is the space required for the hash table. If a set has <code>10^9</code> elements, and each element takes up even just 1 byte of space, that's still a 1 GB hash table!</p>
      <li>
        <p>So let's reduce the number of buckets... to 1! What would the complexity of <code>getValue</code> be if <i>n</i> elements were hashed to the same bucket?</p>
        <p>
          <optionresponse>
            <optioninput options="('O(1)','O(log(n))','O(n)','O(n log(n))','O(n^2)')" correct="O(n)"/>
          </optionresponse>
        </p>
      </li>
      <solution>
        <div class="detailed-solution">
          <p>
            <b>Explanation:</b>
          </p>
          <p>With only one key, all values are part of a very long list! Looking up one value means you must traverse the entire list, at worst case.</p>
        </div>
      </solution>
    </ol>
    <p>OK, so we save on space, but lookup takes forever. So, when creating hash tables, we try to optimize both the size of the table (as small as possible) and lookup time for elements (as short as possible). It turns out that by making the hash table large enough, we can reduce the number of collisions sufficiently to allow us to treat the complexity of lookup as almost O(1).  I.e. we can trade space for time. But what is the tradeoff?  We'll look at this using the tools of probability, in the next problem.</p>
  </text>
</problem>
